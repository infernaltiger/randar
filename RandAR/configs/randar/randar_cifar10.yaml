ar_model:
  target: RandAR_in.model.randar_gpt.RandARTransformer
  params:
    n_layer: 36
    n_head: 20
    dim: 1280
    model_type: c2i
    vocab_size: 512
    block_size: 64 # latent_size ** 2
    num_classes: 10
    cls_token_num: 1
    resid_dropout_p: 0.1
    ffn_dropout_p: 0.1
    drop_path_rate: 0.0
    token_dropout_p: 0.1
    position_order: random # or scan
    grad_checkpointing: True
    zero_class_qk: True
    num_inference_steps: 88
  
dataset:
  target: RandAR_in.dataset.imagenet.INatLatentDataset
  params:
    root_dir: \data\latents_cifar_10\cifar10-cifar10-cifar10-32_codes

tokenizer:
  target: RandAR_in.tokenizer.model.Model
  params:
    batch_size: 256
    num_training_updates: 15000
    num_hiddens: 128
    num_residual_hiddens: 32
    num_residual_layers: 2
    embedding_dim: 64
    num_embeddings: 512
    commitment_cost: 0.25
    decay: 0.99
    learning_rate: 1e-3

accelerator:
  gradient_accumulation_steps: 1
  mixed_precision: bf16
  log_with: wandb

optimizer:
  lr: 0.0004
  weight_decay: 0.05 # 5e-2
  beta1: 0.9
  beta2: 0.95
  max_grad_norm: 1.0
  skip_grad_iter: 100
  skip_grad_norm: 10

lr_scheduler:
  type: cosine
  warm_up_iters: 50000
  min_lr_ratio: 0.05
  num_cycles: 0.5

# training related parameters
exp_name: "randar_xl_0.7b"
max_iters: 360000 # 100k firt
global_batch_size: 32
ema: False 
ema_decay: 0.9999

num_workers: 8
log_every: 100
ckpt_every: 10000
visualize_every: 10 # every 10 log intervals, do one visualization
keep_last_k: -1
resume_from: null # not used, might remove
global_seed: 0

results_dir: "results/" # will append exp_name later => results_dir/exp_name
gpt_ckpt: None # path to resume from

# for eval fid:
vq_ckpt: "./checkpoints/vq_ds16_c2i.pt"

fid_ref_path: "/mnt/localssd/dataset/VIRTUAL_imagenet256_labeled.npz"
fid_save_dir: "/mnt/localssd/results_tmp/"
fid_num_samples: 10000
fid_eval_every: 500000

# for wandb
wandb_entity: "RandAR"
wandb_offline: False
